import streamlit as st
import numpy as np
import librosa
import matplotlib.pyplot as plt
import whisper
import tempfile
import soundfile as sf
from audio_recorder_streamlit import audio_recorder

st.set_page_config(page_title="ï¼éŸ³å£°åˆ†æã‚¢ãƒ—ãƒªï¼", layout="wide")
st.markdown("## ğŸ—£ï¸ éŸ³å£°åˆ†æã‚¢ãƒ—ãƒª") 

# ===== å…±é€šé–¢æ•° =====

def convert_to_wav(uploaded_file):
    tmp_path = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
    with open(tmp_path, "wb") as f:
        f.write(uploaded_file.read())
    return tmp_path

def analyze_features(y, sr):
    duration = librosa.get_duration(y=y, sr=sr)
    rms = librosa.feature.rms(y=y)[0]
    pitch = librosa.yin(y, fmin=50, fmax=500, sr=sr)
    clarity = librosa.feature.spectral_flatness(y=y)[0]
    return {
        "duration": duration,
        "rms": rms,
        "pitch": pitch,
        "clarity": clarity,
        "rms_mean": np.mean(rms),
        "pitch_mean": np.mean(pitch),
        "pitch_std": np.std(pitch),
        "clarity_mean": np.mean(clarity),
    }

def generate_feedback(feat):
    fb = []
    if feat["rms_mean"] < 0.01:
        fb.append("å£°é‡ãŒæ§ãˆã‚ã§ã™ã€‚")
    elif feat["rms_mean"] > 0.03:
        fb.append("ååˆ†ãªå£°é‡ãŒã‚ã‚Šã€åŠ›å¼·ã„ç™ºè©±ã§ã™ã€‚")
    else:
        fb.append("é©åº¦ãªéŸ³é‡ã§è©±ã›ã¦ã„ã¾ã™ã€‚")

    if feat["pitch_mean"] < 110:
        fb.append("ã‚„ã‚„ä½ã‚ã®å£°ã§ã™ã€‚")
    elif feat["pitch_mean"] > 250:
        fb.append("æ¯”è¼ƒçš„é«˜ã‚ã®å£°ã§ã™ã€‚")
    else:
        fb.append("å®‰å®šã—ãŸéŸ³ç¨‹ã§ã™ã€‚")

    if feat["clarity_mean"] < 0.2:
        fb.append("ç™ºè©±ãŒæ˜ç­ã§èãå–ã‚Šã‚„ã™ã„ã§ã™ã€‚")
    elif feat["clarity_mean"] > 0.4:
        fb.append("ã‚„ã‚„ã“ã‚‚ã£ãŸéŸ³ã«èã“ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

    return " ".join(fb)

# ================= éŒ²éŸ³ï¼†Whisperè§£æ =================

st.header("ğŸ™ï¸ éŒ²éŸ³ï¼†æ–‡å­—èµ·ã“ã—")
st.markdown("##### â†“â†“ã‚’æŠ¼ã—ã¦é–‹å§‹ãƒ»åœæ­¢ã‚’æ“ä½œã—ã¾ã™")

wav_audio = audio_recorder(pause_threshold=8.0, sample_rate=16000)

if wav_audio is None:
    st.info("ğŸŸ¢ ãƒã‚¤ã‚¯ãŒé»’ã§å¾…æ©Ÿä¸­â€¦èµ¤ã§éŒ²éŸ³ä¸­ã§ã™")
else:
    st.success("ğŸ”´ éŒ²éŸ³å®Œäº†ï¼å†ç”Ÿãƒ»ä¿å­˜ãƒ»åˆ†æã§ãã¾ã™")

if wav_audio:
    st.audio(wav_audio, format="audio/wav")
    st.download_button("â¬‡ï¸ ã“ã“ã‹ã‚‰éŒ²éŸ³ã‚’ä¿å­˜ã§ãã¾ã™", wav_audio, file_name="recorded.wav")

    if st.button("ğŸ“Š éŒ²éŸ³éŸ³å£°ã‚’éŸ³éŸ¿çš„ã«åˆ†æã™ã‚‹"):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_audio:
            tmp_audio.write(wav_audio)
            tmp_audio.flush()
            y_rec, sr_rec = librosa.load(tmp_audio.name, sr=None)

        feat_rec = analyze_features(y_rec, sr_rec)

        st.subheader("ğŸ“Š éŒ²éŸ³éŸ³å£°ã®éŸ³éŸ¿æŒ‡æ¨™")
        col1, col2, col3 = st.columns(3)
        col1.metric("â± é•·ã•", f"{feat_rec['duration']:.2f}s")
        col2.metric("ğŸ”Š å¹³å‡éŸ³é‡", f"{feat_rec['rms_mean']:.4f}")
        col3.metric("ğŸµ å¹³å‡ãƒ”ãƒƒãƒ", f"{feat_rec['pitch_mean']:.2f}Hz")
        st.metric("ğŸ—£ æ˜ç­åº¦", f"{feat_rec['clarity_mean']:.4f}")
        st.info(generate_feedback(feat_rec))

        st.subheader("ğŸ“ˆ ãƒ”ãƒƒãƒãƒ»éŸ³é‡ã‚°ãƒ©ãƒ•ï¼ˆéŒ²éŸ³éŸ³å£°ï¼‰")
        fig_rec, ax = plt.subplots(2, 1, figsize=(10, 5), sharex=True)
        ax[0].plot(librosa.times_like(feat_rec["rms"], sr=sr_rec), feat_rec["rms"], color="blue")
        ax[1].plot(librosa.times_like(feat_rec["pitch"], sr=sr_rec), feat_rec["pitch"], color="green")
        ax[1].set_xlabel("Time(s)")
        st.pyplot(fig_rec)

    if st.button("ğŸ” Whisperæ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œã™ã‚‹"):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp.write(wav_audio)
            tmp.flush()
            y, sr = librosa.load(tmp.name, sr=None)

        with st.spinner("Whisperã§æ–‡å­—èµ·ã“ã—ä¸­..."):
            model = whisper.load_model("small")
            result = model.transcribe(tmp.name, language="ja")

        st.subheader("ğŸ“ Whisperæ–‡å­—èµ·ã“ã—")
        st.write(result["text"])

        st.subheader("ğŸ“‹ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè©•ä¾¡")
        seg_data = []
        for seg in result["segments"]:
            s_start = seg["start"]
            s_end = seg["end"]
            dur = s_end - s_start
            text = seg["text"].strip()
            words = len(text.replace("ã€€", " ").split())
            rate = round(words / dur, 2) if dur > 0 else 0

            y_part = y[int(s_start * sr):int(s_end * sr)]
            f0 = librosa.yin(y_part, fmin=50, fmax=500, sr=sr)
            f0_mean = round(np.mean(f0), 1)
            f0_std = round(np.std(f0), 1)
            rms = librosa.feature.rms(y=y_part)[0]
            rms_mean = round(np.mean(rms), 4)
            energy = rms < 0.01
            pause_ratio = round(np.sum(energy) / len(rms) * 100, 1)

            comments = []
            if rate < 2.5:
                comments.append("ã‚„ã‚„ã‚†ã£ãã‚Š")
            elif rate > 5:
                comments.append("é€Ÿã„ãƒšãƒ¼ã‚¹")
            if f0_std < 15:
                comments.append("å˜èª¿ãªæŠ‘æº")
            elif f0_std > 30:
                comments.append("æŠ‘æºãŒè±Šã‹")
            if pause_ratio > 30:
                comments.append("ãƒãƒ¼ã‚ºå¤šã‚")
            if not comments:
                comments.append("æ¯”è¼ƒçš„å®‰å®š")

            seg_data.append({
                "åŒºé–“": f"{s_start:.1f}-{s_end:.1f}s",
                "èªæ•°": words,
                "è©±é€Ÿ": f"{rate}èª/ç§’",
                "F0å¹³å‡": f"{f0_mean}Hz",
                "F0ã°ã‚‰ã¤ã": f"{f0_std}Hz",
                "ç„¡éŸ³ç‡": f"{pause_ratio}%",
                "éŸ³é‡": rms_mean,
                "å†…å®¹": text,
                "ã‚³ãƒ¡ãƒ³ãƒˆ": "ã€".join(comments)
            })

        st.dataframe(seg_data, use_container_width=True)

# ========== å˜ä½“éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ ==========
st.header("ğŸ“‚ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ")

uploaded_file = st.file_uploader("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆWAVæ¨å¥¨ï¼‰", type=["wav"])
if uploaded_file:
    st.audio(uploaded_file)
    path = convert_to_wav(uploaded_file)
    y, sr = librosa.load(path, sr=None)
    feat = analyze_features(y, sr)

    st.subheader("ğŸ“Š éŸ³éŸ¿æŒ‡æ¨™")
    col1, col2, col3 = st.columns(3)
    col1.metric("â± é•·ã•", f"{feat['duration']:.2f}s")
    col2.metric("ğŸ”Š å¹³å‡éŸ³é‡", f"{feat['rms_mean']:.4f}")
    col3.metric("ğŸµ å¹³å‡ãƒ”ãƒƒãƒ", f"{feat['pitch_mean']:.2f}Hz")
    st.metric("ğŸ—£ æ˜ç­åº¦", f"{feat['clarity_mean']:.4f}")
    st.info(generate_feedback(feat))

    st.subheader("ğŸ“ˆ ãƒ”ãƒƒãƒãƒ»éŸ³é‡ã‚°ãƒ©ãƒ•")
    fig, ax = plt.subplots(2, 1, figsize=(10, 5), sharex=True)
    ax[0].plot(librosa.times_like(feat["rms"], sr=sr), feat["rms"], color="blue")
    ax[1].plot(librosa.times_like(feat["pitch"], sr=sr), feat["pitch"], color="green")
    ax[1].set_xlabel("æ™‚é–“ï¼ˆç§’ï¼‰")
    st.pyplot(fig)

# ========== éŸ³å£°A/B æ¯”è¼ƒåˆ†æ ==========
st.header("ğŸ“‚ éŸ³å£°ã®æ¯”è¼ƒåˆ†æ")

def compare_features(fa, fb):
    def pct(a, b):
        return f"{(b - a) / a * 100:+.1f}%" if a != 0 else "N/A"
    return {
        "éŸ³å£°é•·": f"{fa['duration']:.2f}s â†’ {fb['duration']:.2f}s",
        "å¹³å‡éŸ³é‡": f"{fa['rms_mean']:.4f} â†’ {fb['rms_mean']:.4f}ï¼ˆ{pct(fa['rms_mean'], fb['rms_mean'])}ï¼‰",
        "å¹³å‡ãƒ”ãƒƒãƒ": f"{fa['pitch_mean']:.2f}Hz â†’ {fb['pitch_mean']:.2f}Hzï¼ˆ{pct(fa['pitch_mean'], fb['pitch_mean'])}ï¼‰",
        "ãƒ”ãƒƒãƒã°ã‚‰ã¤ã": f"{fa['pitch_std']:.2f}Hz â†’ {fb['pitch_std']:.2f}Hzï¼ˆ{pct(fa['pitch_std'], fb['pitch_std'])}ï¼‰",
        "æ˜ç­åº¦": f"{fa['clarity_mean']:.4f} â†’ {fb['clarity_mean']:.4f}ï¼ˆ{pct(fa['clarity_mean'], fb['clarity_mean'])}ï¼‰",
    }

def compare_feedback(fa, fb):
    msg = []
    if fb["rms_mean"] > fa["rms_mean"]:
        msg.append("Bã®æ–¹ãŒå£°é‡ãŒå¤§ãã„ã§ã™ã€‚")
    else:
        msg.append("Aã®æ–¹ãŒå£°é‡ãŒå¤§ãã„ã§ã™ã€‚")
    if fb["pitch_std"] < fa["pitch_std"]:
        msg.append("Bã¯éŸ³ç¨‹ãŒå®‰å®šã—ã¦ã„ã¾ã™ã€‚")
    else:
        msg.append("Aã®æ–¹ãŒæŠ‘æºãŒã‚ã‚Šã¾ã™ã€‚")
    if fb["clarity_mean"] < fa["clarity_mean"]:
        msg.append("Bã¯æ˜ç­åº¦ãŒé«˜ã‚ã§ã™ã€‚")
    else:
        msg.append("Aã¯æ˜ç­åº¦ãŒé«˜ã‚ã§ã™ã€‚")
    return " ".join(msg)

col1, col2 = st.columns(2)
file_a = col1.file_uploader("éŸ³å£°A", type=["wav"], key="compare_a")
file_b = col2.file_uploader("éŸ³å£°B", type=["wav"], key="compare_b")

if file_a and file_b:
    st.audio(file_a)
    st.audio(file_b)
    path_a = convert_to_wav(file_a)
    path_b = convert_to_wav(file_b)
    ya, sr_a = librosa.load(path_a, sr=None)
    yb, sr_b = librosa.load(path_b, sr=None)
    fa = analyze_features(ya, sr_a)
    fb = analyze_features(yb, sr_b)

    st.subheader("ğŸ“Š æŒ‡æ¨™ã®æ¯”è¼ƒ")
    diffs = compare_features(fa, fb)
    for k, v in diffs.items():
        st.markdown(f"**{k}**: {v}")

    st.subheader("ğŸ“ ã‚³ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹æ¯”è¼ƒ")
    st.info(compare_feedback(fa, fb))

    st.subheader("ğŸ“ˆ æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ï¼šéŸ³é‡ / ãƒ”ãƒƒãƒ / æ˜ç­åº¦ / æŒ¯å¹…")
    fig, axes = plt.subplots(4, 1, figsize=(10, 8), sharex=False)

    t_rms_a = librosa.times_like(fa["rms"], sr=sr_a)
    t_rms_b = librosa.times_like(fb["rms"], sr=sr_b)
    t_pitch_a = librosa.times_like(fa["pitch"], sr=sr_a)
    t_pitch_b = librosa.times_like(fb["pitch"], sr=sr_b)
    t_flat_a = librosa.times_like(fa["clarity"], sr=sr_a)
    t_flat_b = librosa.times_like(fb["clarity"], sr=sr_b)
    t_amp_a = np.linspace(0, len(ya) / sr_a, len(ya))
    t_amp_b = np.linspace(0, len(yb) / sr_b, len(yb))

    axes[0].plot(t_rms_a, fa["rms"], label="A", color="blue")
    axes[0].plot(t_rms_b, fb["rms"], label="B", color="orange", linestyle="--")
    axes[0].set_ylabel("RMS (Volume)")
    axes[0].legend()

    axes[1].plot(t_pitch_a, fa["pitch"], label="A", color="blue")
    axes[1].plot(t_pitch_b, fb["pitch"], label="B", color="orange", linestyle="--")
    axes[1].set_ylabel("Pitch (Hz)")
    axes[1].legend()

    axes[2].plot(t_flat_a, fa["clarity"], label="A", color="blue")
    axes[2].plot(t_flat_b, fb["clarity"], label="B", color="orange", linestyle="--")
    axes[2].set_ylabel("Spectral Flatness")
    axes[2].legend()

    axes[3].plot(t_amp_a, ya, label="A", alpha=0.6, color="blue")
    axes[3].plot(t_amp_b, yb, label="B", alpha=0.6, color="orange")
    axes[3].set_ylabel("Amplitude")
    axes[3].set_xlabel("Time (sec)")
    axes[3].legend()

    st.pyplot(fig)

    st.markdown("""
**ğŸ§¾ ã‚°ãƒ©ãƒ•ã®ãƒ©ãƒ™ãƒ«èª¬æ˜ï¼š**

- **RMS (Volume)**ï¼šéŸ³é‡ï¼ˆæŒ¯å¹…ã®å¹³å‡å¼·åº¦ï¼‰  
- **Pitch (Hz)**ï¼šåŸºæœ¬å‘¨æ³¢æ•°ï¼ˆå£°ã®é«˜ã•ï¼‰  
- **Spectral Flatness**ï¼šæ˜ç­åº¦ã®æŒ‡æ¨™ï¼ˆãƒã‚¤ã‚ºçš„ã‹ã©ã†ã‹ï¼‰  
- **Amplitude**ï¼šç”Ÿæ³¢å½¢ã®æŒ¯å¹…ï¼ˆç¬é–“çš„ãªå¤‰å‹•ï¼‰
""")

# ===== åŒºé–“åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆéŸ³å£°ã‚ã‚Šã®å ´åˆã®ã¿è¡¨ç¤ºï¼‰ =====

def generate_natural_feedback(f1, f2, centroid_mean, bandwidth_mean, slope, flatness_mean):
    feedback = []
    if f1 and f2:
        if f1 > 800 or f2 < 1000:
            feedback.append("æ¯éŸ³ã®æ˜ç­åº¦ãŒä½ãã€ç™ºéŸ³ãŒã“ã‚‚ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    if centroid_mean < 1200:
        feedback.append("éŸ³å£°å…¨ä½“ãŒã“ã‚‚ã£ãŸå°è±¡ã‚’ä¸ãˆã‚‹ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†å¸ƒã§ã™ã€‚")
    elif centroid_mean > 2500:
        feedback.append("æ˜ç­ã§é‹­ã„å°è±¡ã®éŸ³å£°ç‰¹æ€§ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚")
    if bandwidth_mean < 300:
        feedback.append("å‘¨æ³¢æ•°ã®åºƒãŒã‚ŠãŒç‹­ãã€ã‚„ã‚„å˜èª¿ãªéŸ³è³ªã§ã™ã€‚")
    if slope < -10:
        feedback.append("é«˜åŸŸã®æ¸›è¡°ãŒå¼·ãã€å£°ã®æŠœã‘ãŒå¼±ãæ„Ÿã˜ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    if flatness_mean > 0.85:
        feedback.append("ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯æˆåˆ†ãŒå°‘ãªãã€ãƒã‚¤ã‚ºçš„ãªå‚¾å‘ãŒå¼·ã„ã§ã™ã€‚")
    return "ğŸ“ éŸ³éŸ¿ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯:\n" + "\n".join(f"- {line}" for line in feedback) if feedback else "éŸ³éŸ¿æŒ‡æ¨™ã«å¤§ããªç•°å¸¸ã¯è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã€‚"

# âœ… éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
if wav_audio is not None and len(wav_audio) > 0:
    st.header("â†”ï¸ ç¯„å›²æŒ‡å®šã—ã¦åˆ†æ")
    st.markdown("#### ğŸ§­ åŒºé–“é¸æŠã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ï¼ˆé–‹å§‹ä½ç½®ã‚’æŒ‡å®šï¼‰")

    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_wav:
        tmp_wav.write(wav_audio)
        tmp_wav.flush()
        y_full, sr_full = librosa.load(tmp_wav.name, sr=None)
        dur_full = librosa.get_duration(y=y_full, sr=sr_full)

    if dur_full < 15.0:
        st.warning("âš ï¸ éŒ²éŸ³ãŒ15ç§’æœªæº€ã®ãŸã‚ã€åŒºé–“åˆ†æã¯å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚")
    else:
        start_sec = st.slider("åˆ†æã™ã‚‹é–‹å§‹ä½ç½®ï¼ˆç§’ï¼‰", 0.0, dur_full - 15.0, step=0.1, format="%.1f ç§’")
        start_sample = int(start_sec * sr_full)
        end_sample = int(start_sample + 15 * sr_full)
        y_seg = y_full[start_sample:end_sample]

        tmp_path = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
        sf.write(tmp_path, y_seg, sr_full)
        with open(tmp_path, "rb") as f:
            seg_bytes = f.read()
        st.audio(seg_bytes, format="audio/wav")

        if st.button("ğŸ” ã“ã®15ç§’åŒºé–“ã‚’åˆ†æã™ã‚‹"):
            feat = analyze_features(y_seg, sr_full)

            st.subheader("ğŸ“Š åŒºé–“ã®æŒ‡æ¨™ï¼ˆ15ç§’é–“ï¼‰")
            col1, col2, col3 = st.columns(3)
            col1.metric("ğŸ”Š å¹³å‡éŸ³é‡", f"{feat['rms_mean']:.4f}")
            col2.metric("ğŸµ å¹³å‡ãƒ”ãƒƒãƒ", f"{feat['pitch_mean']:.2f}Hz")
            col3.metric("ğŸ—£ æ˜ç­åº¦", f"{feat['clarity_mean']:.4f}")
            st.info(generate_feedback(feat))

            st.subheader("ğŸ“ˆ åŒºé–“ã®éŸ³é‡ãƒ»ãƒ”ãƒƒãƒæ¨ç§»")
            fig, ax = plt.subplots(2, 1, figsize=(10, 5), sharex=True)
            t_rms = librosa.times_like(feat["rms"], sr=sr_full)
            t_pitch = librosa.times_like(feat["pitch"], sr=sr_full)
            ax[0].plot(t_rms[: len(feat["rms"])], feat["rms"], color="blue")
            ax[0].set_ylabel("RMS")
            ax[1].plot(t_pitch[: len(feat["pitch"])], feat["pitch"], color="green")
            ax[1].set_ylabel("Pitch (Hz)")
            ax[1].set_xlabel("Timeï¼ˆsï¼‰")
            st.pyplot(fig)

            st.subheader("ğŸ§ª æ‹¡å¼µéŸ³éŸ¿æŒ‡æ¨™")

            mfcc = librosa.feature.mfcc(y=y_seg, sr=sr_full, n_mfcc=13)
            mfcc_mean = np.mean(mfcc, axis=1)
            mfcc_std = np.std(mfcc, axis=1)

            mid = len(y_seg) // 2
            frame = y_seg[mid - 512: mid + 512] * np.hamming(1024)
            lpc_order = int(sr_full / 1000) + 2
            lpc_coeffs = librosa.lpc(frame, order=lpc_order)
            roots = np.roots(lpc_coeffs)
            roots = roots[np.imag(roots) >= 0]
            freqs = np.angle(roots) * (sr_full / (2 * np.pi))
            formants = np.sort(freqs[freqs < sr_full / 2])
            f1 = round(formants[0], 1) if len(formants) > 0 else None
            f2 = round(formants[1], 1) if len(formants) > 1 else None

            centroid = librosa.feature.spectral_centroid(y=y_seg, sr=sr_full)[0]
            bandwidth = librosa.feature.spectral_bandwidth(y=y_seg, sr=sr_full)[0]
            flatness = librosa.feature.spectral_flatness(y=y_seg)[0]
            centroid_mean = round(np.mean(centroid), 1)
            bandwidth_mean = round(np.mean(bandwidth), 1)
            flatness_mean = round(np.mean(flatness), 4)

            S = np.abs(librosa.stft(y_seg, n_fft=1024))
            S_db = librosa.amplitude_to_db(S, ref=np.max)
            freqs_db = librosa.fft_frequencies(sr=sr_full)
            freqs_db = freqs_db[: S_db.shape[0]]
            mean_spectrum = np.mean(S_db, axis=1)
            slope = np.polyfit(freqs_db, mean_spectrum, deg=1)[0]

            st.markdown(f"- **ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå‘¨æ³¢æ•°**ï¼šF1 = {f1} Hz, F2 = {f2} Hz")
            st.markdown(f"- **ã‚¹ãƒšã‚¯ãƒˆãƒ«ä¸­å¿ƒå‘¨æ³¢æ•°**ï¼š{centroid_mean:.2f} Hz")
            st.markdown(f"- **ã‚¹ãƒšã‚¯ãƒˆãƒ«å¸¯åŸŸå¹…**ï¼š{bandwidth_mean:.2f} Hz")
            st.markdown(f"- **ã‚¹ãƒšã‚¯ãƒˆãƒ«å‚¾æ–œ**ï¼š{slope:.2f} dB/oct")
            st.markdown(f"- **ã‚¹ãƒšã‚¯ãƒˆãƒ«å¹³å¦åº¦**ï¼š{flatness_mean:.3f}")

            st.markdown("#### ğŸ—’ éŸ³éŸ¿ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯")
            st.info(generate_natural_feedback(f1, f2, centroid_mean, bandwidth_mean, slope, flatness_mean))

            st.markdown("##### MFCC Mean & Variation Radar Chart")

            def plot_combined_radar(mean_vals, std_vals):
                labels = [f"MFCC{i+1}" for i in range(len(mean_vals))]
                angles = np.linspace(0, 2 * np.pi, len(mean_vals), endpoint=False).tolist()
                mean_vals += mean_vals[:1]
                std_vals += std_vals[:1]
                angles += angles[:1]

                fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
                ax.plot(angles, mean_vals, color="blue", linewidth=2, label="Mean")
                ax.fill(angles, mean_vals, color="blue", alpha=0.25)
                ax.plot(angles, std_vals, color="orange", linewidth=2, label="Variation")
                ax.fill(angles, std_vals, color="orange", alpha=0.25)
                ax.set_thetagrids(np.degrees(angles[:-1]), labels)
                ax.set_title("MFCC Mean & Variation (Normalized)")
                ax.grid(True)
                ax.legend(loc="upper right", bbox_to_anchor=(1.2, 1.2))
                st.pyplot(fig)

            mfcc_mean_norm = (mfcc_mean - np.min(mfcc_mean)) / (np.max(mfcc_mean) - np.min(mfcc_mean) + 1e-6)
            mfcc_std_norm = (mfcc_std - np.min(mfcc_std)) / (np.max(mfcc_std) - np.min(mfcc_std) + 1e-6)
            plot_combined_radar(mfcc_mean_norm.tolist(), mfcc_std_norm.tolist())

            st.markdown("""
**ğŸ§¾ MFCCãƒ©ãƒ™ãƒ«ã®èª¬æ˜ï¼š**

- **MFCC1ã€œ13** ã¯éŸ³å£°ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®å½¢çŠ¶ã‚’è¦ç´„ã—ãŸç‰¹å¾´é‡ã§ã™  
- **MFCC Mean** ã¯å¹³å‡çš„ãªéŸ³éŸ¿ç‰¹æ€§ã‚’ç¤ºã—ã€å£°è³ªã‚„æ¯éŸ³åˆ†å¸ƒã®å‚¾å‘  
- **MFCC Variation** ã¯éŸ³éŸ¿ã®å¤‰å‹•æ€§ï¼ˆå£°ã®æºã‚‰ãã‚„å¤šæ§˜æ€§ï¼‰ã‚’ç¤ºã—ã¾ã™  

â€» é’ï¼MFCCå¹³å‡â€ƒâ€ƒã‚ªãƒ¬ãƒ³ã‚¸ï¼MFCCå¤‰å‹•ï¼ˆåŒä¸€ã‚°ãƒ©ãƒ•å†…ã«é‡ã­ã¦è¡¨ç¤ºï¼‰
""")
